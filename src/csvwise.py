#!/usr/bin/env python3
"""
csvwise - AI-Powered CSV Data Analyst CLI
Ask questions about your CSV data in natural language.
"""

import argparse
import csv
import io
import json
import os
import subprocess
import sys
import tempfile
from datetime import datetime
from pathlib import Path

# ---------------------------------------------------------------------------
# Constants
# ---------------------------------------------------------------------------
VERSION = "0.1.0"
MAX_PREVIEW_ROWS = 20          # rows sent to LLM for schema understanding
MAX_ANALYSIS_ROWS = 200        # rows sent for deep analysis
MAX_CELL_LEN = 200             # truncate long cell values
STATE_DIR = Path.home() / ".csvwise"
HISTORY_FILE = STATE_DIR / "history.json"

# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

def ensure_state_dir():
    STATE_DIR.mkdir(parents=True, exist_ok=True)


def load_csv(path: str):
    """Load CSV and return (headers, rows) with basic validation."""
    p = Path(path)
    if not p.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        sys.exit(1)
    if p.suffix.lower() not in (".csv", ".tsv", ".txt"):
        print(f"âš ï¸  æ–‡ä»¶ç±»å‹ {p.suffix} å¯èƒ½ä¸æ˜¯ CSVï¼Œå°è¯•åŠ è½½ä¸­...")

    # Detect encoding
    encodings = ["utf-8", "utf-8-sig", "gbk", "gb2312", "latin-1"]
    raw = p.read_bytes()
    text = None
    for enc in encodings:
        try:
            text = raw.decode(enc)
            break
        except (UnicodeDecodeError, LookupError):
            continue
    if text is None:
        print("âŒ æ— æ³•è§£ç æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ç¼–ç ")
        sys.exit(1)

    # Detect delimiter
    sniffer = csv.Sniffer()
    try:
        dialect = sniffer.sniff(text[:4096])
        delimiter = dialect.delimiter
    except csv.Error:
        delimiter = "," if "," in text[:1024] else "\t"

    reader = csv.reader(io.StringIO(text), delimiter=delimiter)
    rows = list(reader)
    if len(rows) < 2:
        print("âŒ CSV æ–‡ä»¶è‡³å°‘éœ€è¦è¡¨å¤´ + 1è¡Œæ•°æ®")
        sys.exit(1)

    headers = rows[0]
    data = rows[1:]
    return headers, data, delimiter


def truncate(s, maxlen=MAX_CELL_LEN):
    s = str(s).strip()
    return s[:maxlen] + "..." if len(s) > maxlen else s


def csv_to_markdown_table(headers, rows, max_rows=None):
    """Convert CSV rows to markdown table string."""
    if max_rows:
        rows = rows[:max_rows]
    lines = []
    lines.append("| " + " | ".join(headers) + " |")
    lines.append("| " + " | ".join(["---"] * len(headers)) + " |")
    for row in rows:
        # Pad or truncate row to match header count
        padded = row + [""] * (len(headers) - len(row))
        padded = padded[:len(headers)]
        lines.append("| " + " | ".join(truncate(c) for c in padded) + " |")
    return "\n".join(lines)


def infer_column_types(headers, data):
    """Infer column types by sampling data."""
    types = {}
    for i, h in enumerate(headers):
        nums = 0
        dates = 0
        total = min(len(data), 50)
        for row in data[:50]:
            if i >= len(row) or not row[i].strip():
                continue
            val = row[i].strip()
            # Try number
            try:
                float(val.replace(",", "").replace("%", ""))
                nums += 1
                continue
            except ValueError:
                pass
            # Try date
            for fmt in ("%Y-%m-%d", "%Y/%m/%d", "%m/%d/%Y", "%d/%m/%Y", "%Y-%m-%d %H:%M:%S"):
                try:
                    datetime.strptime(val, fmt)
                    dates += 1
                    break
                except ValueError:
                    continue
        if total == 0:
            types[h] = "empty"
        elif nums / max(total, 1) > 0.7:
            types[h] = "numeric"
        elif dates / max(total, 1) > 0.5:
            types[h] = "date"
        else:
            types[h] = "text"
    return types


def build_schema_prompt(headers, data, col_types):
    """Build a schema description for the LLM."""
    lines = ["## æ•°æ®é›†æ¦‚è¦", f"- æ€»è¡Œæ•°: {len(data)}", f"- åˆ—æ•°: {len(headers)}", ""]
    lines.append("## åˆ—ä¿¡æ¯")
    for h in headers:
        t = col_types.get(h, "unknown")
        # Get sample unique values
        idx = headers.index(h)
        vals = set()
        for row in data[:100]:
            if idx < len(row) and row[idx].strip():
                vals.add(truncate(row[idx], 50))
            if len(vals) >= 5:
                break
        sample = ", ".join(list(vals)[:5])
        lines.append(f"- **{h}** (ç±»å‹: {t}) â€” ç¤ºä¾‹å€¼: {sample}")
    return "\n".join(lines)


def compute_basic_stats(headers, data, col_types):
    """Compute basic statistics for numeric columns."""
    stats = {}
    for h in headers:
        if col_types.get(h) != "numeric":
            continue
        idx = headers.index(h)
        values = []
        for row in data:
            if idx < len(row) and row[idx].strip():
                try:
                    values.append(float(row[idx].strip().replace(",", "").replace("%", "")))
                except ValueError:
                    pass
        if not values:
            continue
        values.sort()
        n = len(values)
        stats[h] = {
            "count": n,
            "min": round(values[0], 4),
            "max": round(values[-1], 4),
            "mean": round(sum(values) / n, 4),
            "median": round(values[n // 2], 4),
            "sum": round(sum(values), 4),
        }
    return stats


def llm_query(prompt: str, timeout: int = 60) -> str:
    """Call gemini CLI for LLM inference."""
    try:
        result = subprocess.run(
            ["gemini", prompt],
            capture_output=True,
            text=True,
            timeout=timeout,
        )
        if result.returncode == 0:
            return result.stdout.strip()
        else:
            # Fallback: try with stdin
            result2 = subprocess.run(
                ["gemini"],
                input=prompt,
                capture_output=True,
                text=True,
                timeout=timeout,
            )
            return result2.stdout.strip() if result2.returncode == 0 else f"âŒ LLM è°ƒç”¨å¤±è´¥: {result.stderr[:200]}"
    except FileNotFoundError:
        return "âŒ æœªæ‰¾åˆ° gemini CLIã€‚è¯·å®‰è£…: npm i -g @anthropic-ai/gemini-cli"
    except subprocess.TimeoutExpired:
        return "âŒ LLM è°ƒç”¨è¶…æ—¶"


def save_history(action: str, file: str, query: str, result_preview: str):
    """Save query history."""
    ensure_state_dir()
    history = []
    if HISTORY_FILE.exists():
        try:
            history = json.loads(HISTORY_FILE.read_text())
        except (json.JSONDecodeError, IOError):
            history = []
    history.append({
        "timestamp": datetime.now().isoformat(),
        "action": action,
        "file": file,
        "query": query,
        "result_preview": result_preview[:200],
    })
    # Keep last 100 entries
    history = history[-100:]
    HISTORY_FILE.write_text(json.dumps(history, ensure_ascii=False, indent=2))


# ---------------------------------------------------------------------------
# Commands
# ---------------------------------------------------------------------------

def cmd_info(args):
    """Show dataset information."""
    headers, data, delim = load_csv(args.file)
    col_types = infer_column_types(headers, data)
    stats = compute_basic_stats(headers, data, col_types)

    print(f"\nğŸ“Š æ•°æ®é›†: {args.file}")
    print(f"   è¡Œæ•°: {len(data):,}  |  åˆ—æ•°: {len(headers)}  |  åˆ†éš”ç¬¦: {repr(delim)}")
    print()

    # Column info
    print("ğŸ“‹ åˆ—ä¿¡æ¯:")
    for h in headers:
        t = col_types.get(h, "unknown")
        emoji = {"numeric": "ğŸ”¢", "date": "ğŸ“…", "text": "ğŸ“", "empty": "â¬œ"}.get(t, "â“")
        line = f"   {emoji} {h} ({t})"
        if h in stats:
            s = stats[h]
            line += f"  â€” min={s['min']}, max={s['max']}, mean={s['mean']}, median={s['median']}"
        print(line)

    # Preview
    print(f"\nğŸ“ƒ å‰ {min(5, len(data))} è¡Œé¢„è§ˆ:")
    print(csv_to_markdown_table(headers, data, max_rows=5))
    print()


def cmd_ask(args):
    """Ask a natural language question about the data."""
    headers, data, delim = load_csv(args.file)
    col_types = infer_column_types(headers, data)
    stats = compute_basic_stats(headers, data, col_types)
    schema = build_schema_prompt(headers, data, col_types)

    # Build data sample
    sample_rows = min(MAX_ANALYSIS_ROWS, len(data))
    table = csv_to_markdown_table(headers, data, max_rows=sample_rows)

    # Stats section
    stats_text = ""
    if stats:
        stats_lines = ["## åŸºç¡€ç»Ÿè®¡"]
        for h, s in stats.items():
            stats_lines.append(f"- {h}: count={s['count']}, min={s['min']}, max={s['max']}, mean={s['mean']}, median={s['median']}, sum={s['sum']}")
        stats_text = "\n".join(stats_lines)

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆã€‚è¯·æ ¹æ®ä»¥ä¸‹ CSV æ•°æ®å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

{schema}

{stats_text}

## æ•°æ®æ ·æœ¬ (å‰ {sample_rows} è¡Œï¼Œå…± {len(data)} è¡Œ)
{table}

## ç”¨æˆ·é—®é¢˜
{args.question}

## å›ç­”è¦æ±‚
1. ç”¨ä¸­æ–‡å›ç­”
2. ç»™å‡ºå…·ä½“æ•°æ®å’Œè®¡ç®—è¿‡ç¨‹
3. å¦‚æœéœ€è¦ï¼Œç”¨ markdown è¡¨æ ¼å±•ç¤ºç»“æœ
4. æŒ‡å‡ºæ•°æ®ä¸­çš„æœ‰è¶£å‘ç°
5. å¦‚æœæ•°æ®ä¸è¶³ä»¥å›ç­”ï¼Œè¯´æ˜åŸå› å¹¶å»ºè®®éœ€è¦ä»€ä¹ˆé¢å¤–æ•°æ®"""

    print(f"\nğŸ¤” åˆ†æä¸­: {args.question}")
    print("â”€" * 60)
    result = llm_query(prompt, timeout=90)
    print(result)
    print("â”€" * 60)

    save_history("ask", args.file, args.question, result)


def cmd_report(args):
    """Generate a comprehensive analysis report."""
    headers, data, delim = load_csv(args.file)
    col_types = infer_column_types(headers, data)
    stats = compute_basic_stats(headers, data, col_types)
    schema = build_schema_prompt(headers, data, col_types)

    sample_rows = min(MAX_ANALYSIS_ROWS, len(data))
    table = csv_to_markdown_table(headers, data, max_rows=sample_rows)

    stats_text = ""
    if stats:
        stats_lines = ["## åŸºç¡€ç»Ÿè®¡"]
        for h, s in stats.items():
            stats_lines.append(f"- {h}: count={s['count']}, min={s['min']}, max={s['max']}, mean={s['mean']}, median={s['median']}, sum={s['sum']}")
        stats_text = "\n".join(stats_lines)

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªèµ„æ·±æ•°æ®åˆ†æå¸ˆã€‚è¯·å¯¹ä»¥ä¸‹ CSV æ•°æ®ç”Ÿæˆä¸€ä»½å…¨é¢çš„åˆ†ææŠ¥å‘Šã€‚

{schema}

{stats_text}

## æ•°æ®æ ·æœ¬ (å‰ {sample_rows} è¡Œï¼Œå…± {len(data)} è¡Œ)
{table}

## æŠ¥å‘Šè¦æ±‚
è¯·ç”Ÿæˆä»¥ä¸‹ç« èŠ‚çš„è¯¦ç»†æŠ¥å‘Šï¼ˆä¸­æ–‡ï¼‰ï¼š

### 1. ğŸ“Š æ•°æ®æ¦‚è§ˆ
- æ•°æ®é›†å¤§å°ã€å®Œæ•´æ€§ã€è´¨é‡è¯„ä¼°

### 2. ğŸ“ˆ å…³é”®å‘ç°
- æœ€é‡è¦çš„ 3-5 ä¸ªå‘ç°
- ç”¨å…·ä½“æ•°æ®æ”¯æ’‘

### 3. ğŸ“‰ è¶‹åŠ¿ä¸æ¨¡å¼
- æ•°æ®ä¸­çš„è¶‹åŠ¿ï¼ˆå¦‚æœ‰æ—¶é—´ç»´åº¦ï¼‰
- åˆ†å¸ƒç‰¹å¾
- å¼‚å¸¸å€¼

### 4. ğŸ”— å…³è”åˆ†æ
- åˆ—ä¹‹é—´çš„å…³ç³»
- æœ‰æ„ä¹‰çš„åˆ†ç»„å¯¹æ¯”

### 5. ğŸ’¡ å»ºè®®ä¸æ´å¯Ÿ
- åŸºäºæ•°æ®çš„å¯è¡Œå»ºè®®
- éœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥çš„æ–¹å‘

### 6. âš ï¸ æ•°æ®å±€é™æ€§
- æ•°æ®çš„ä¸è¶³ä¹‹å¤„
- æ”¹è¿›å»ºè®®

ç”¨ markdown æ ¼å¼è¾“å‡ºï¼ŒåŒ…å«è¡¨æ ¼å’Œåˆ—è¡¨ã€‚"""

    print(f"\nğŸ“ ç”Ÿæˆåˆ†ææŠ¥å‘Š: {args.file}")
    print("â•" * 60)
    result = llm_query(prompt, timeout=120)
    print(result)
    print("â•" * 60)

    # Save report
    if args.output:
        out_path = Path(args.output)
        report_content = f"# æ•°æ®åˆ†ææŠ¥å‘Š: {args.file}\n\n"
        report_content += f"_ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}_\n\n"
        report_content += result
        out_path.write_text(report_content, encoding="utf-8")
        print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜: {out_path}")

    save_history("report", args.file, "full_report", result)


def cmd_clean(args):
    """AI-suggested data cleaning recommendations."""
    headers, data, delim = load_csv(args.file)
    col_types = infer_column_types(headers, data)
    schema = build_schema_prompt(headers, data, col_types)

    # Analyze data quality
    quality = {}
    for i, h in enumerate(headers):
        empty = sum(1 for row in data if i >= len(row) or not row[i].strip())
        duplicates = len(data) - len(set(row[i] if i < len(row) else "" for row in data))
        quality[h] = {"empty_count": empty, "empty_pct": round(empty / len(data) * 100, 1), "approx_duplicates": duplicates}

    quality_text = "## æ•°æ®è´¨é‡æ£€æŸ¥\n"
    for h, q in quality.items():
        flags = []
        if q["empty_pct"] > 5:
            flags.append(f"âš ï¸ ç©ºå€¼ {q['empty_count']}ä¸ª ({q['empty_pct']}%)")
        if q["approx_duplicates"] > len(data) * 0.3:
            flags.append(f"ğŸ”„ é‡å¤å€¼è¾ƒå¤š")
        flag_str = " | ".join(flags) if flags else "âœ…"
        quality_text += f"- {h}: {flag_str}\n"

    table = csv_to_markdown_table(headers, data, max_rows=20)

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ•°æ®æ¸…æ´—ä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹æ•°æ®é›†çš„è´¨é‡é—®é¢˜å¹¶ç»™å‡ºæ¸…æ´—å»ºè®®ã€‚

{schema}

{quality_text}

## æ•°æ®æ ·æœ¬
{table}

## è¯·è¾“å‡º
1. ğŸ” **å‘ç°çš„é—®é¢˜** â€” ç©ºå€¼ã€å¼‚å¸¸å€¼ã€æ ¼å¼ä¸ä¸€è‡´ã€ç¼–ç é—®é¢˜ç­‰
2. ğŸ› ï¸ **æ¸…æ´—å»ºè®®** â€” å…·ä½“çš„å¤„ç†æ–¹æ¡ˆï¼ˆå¡«å……ç­–ç•¥ã€åˆ é™¤ç­–ç•¥ã€æ ¼å¼æ ‡å‡†åŒ–ç­‰ï¼‰
3. ğŸ“Š **æ¸…æ´—åé¢„æœŸæ•ˆæœ** â€” æ•°æ®è´¨é‡æå‡é¢„ä¼°
4. ğŸ **Python ä»£ç ç‰‡æ®µ** â€” å¯ç›´æ¥è¿è¡Œçš„ pandas æ¸…æ´—ä»£ç 

ç”¨ä¸­æ–‡å›ç­”ã€‚"""

    print(f"\nğŸ§¹ æ•°æ®è´¨é‡åˆ†æ: {args.file}")
    print("â”€" * 60)
    result = llm_query(prompt, timeout=90)
    print(result)
    print("â”€" * 60)

    save_history("clean", args.file, "clean_analysis", result)


def cmd_plot(args):
    """Generate a Python matplotlib plotting script."""
    headers, data, delim = load_csv(args.file)
    col_types = infer_column_types(headers, data)
    schema = build_schema_prompt(headers, data, col_types)

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ•°æ®å¯è§†åŒ–ä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„æè¿°ç”Ÿæˆ Python matplotlib ç»‘å›¾ä»£ç ã€‚

{schema}

## ç”¨æˆ·è¦æ±‚
{args.description}

## ä»£ç è¦æ±‚
1. ä½¿ç”¨ pandas + matplotlib
2. ä¸­æ–‡æ ‡é¢˜å’Œæ ‡ç­¾
3. ç¾è§‚çš„é…è‰²æ–¹æ¡ˆ
4. ä»£ç å¯ç›´æ¥è¿è¡Œ
5. è¯»å–æ–‡ä»¶è·¯å¾„: {os.path.abspath(args.file)}
6. ä¿å­˜å›¾ç‰‡åˆ°åŒç›®å½•
7. æ‰“å°ä¿å­˜è·¯å¾„

åªè¾“å‡º Python ä»£ç ï¼Œä¸è¦è§£é‡Šã€‚ç”¨ ```python ``` åŒ…è£¹ã€‚"""

    print(f"\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–ä»£ç ...")
    print("â”€" * 60)
    result = llm_query(prompt, timeout=60)

    # Extract code block
    code = result
    if "```python" in result:
        code = result.split("```python")[1].split("```")[0].strip()
    elif "```" in result:
        code = result.split("```")[1].split("```")[0].strip()

    # Save script
    script_path = Path(args.file).parent / f"plot_{Path(args.file).stem}.py"
    script_path.write_text(code, encoding="utf-8")
    print(f"ğŸ“ ç»˜å›¾è„šæœ¬å·²ä¿å­˜: {script_path}")

    if args.run:
        print("\nğŸš€ è¿è¡Œç»˜å›¾è„šæœ¬...")
        try:
            subprocess.run([sys.executable, str(script_path)], timeout=30, check=True)
            print("âœ… å›¾è¡¨ç”ŸæˆæˆåŠŸ!")
        except subprocess.CalledProcessError as e:
            print(f"âŒ è¿è¡Œå¤±è´¥: {e}")
        except subprocess.TimeoutExpired:
            print("âŒ è¿è¡Œè¶…æ—¶")
    else:
        print(f"ğŸ’¡ è¿è¡Œ: python {script_path}")

    print("â”€" * 60)
    save_history("plot", args.file, args.description, code[:200])


def cmd_query(args):
    """Execute a SQL-like query on the CSV (via pandas)."""
    headers, data, delim = load_csv(args.file)
    col_types = infer_column_types(headers, data)
    schema = build_schema_prompt(headers, data, col_types)

    prompt = f"""ä½ æ˜¯ä¸€ä¸ª Python pandas ä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„æŸ¥è¯¢éœ€æ±‚ç”Ÿæˆ pandas ä»£ç ã€‚

{schema}

## ç”¨æˆ·æŸ¥è¯¢
{args.sql}

## ä»£ç è¦æ±‚
1. è¯»å– CSV: pd.read_csv("{os.path.abspath(args.file)}")
2. æ‰§è¡ŒæŸ¥è¯¢
3. æ‰“å°ç»“æœï¼ˆç”¨ tabulate æˆ– to_markdown æ ¼å¼åŒ–ï¼‰
4. å¦‚æœç»“æœæ˜¯æ•°å€¼ï¼Œç›´æ¥æ‰“å°
5. åªè¾“å‡ºå¯æ‰§è¡Œçš„ Python ä»£ç 

åªè¾“å‡ºä»£ç ï¼Œç”¨ ```python ``` åŒ…è£¹ã€‚"""

    result = llm_query(prompt, timeout=60)

    code = result
    if "```python" in result:
        code = result.split("```python")[1].split("```")[0].strip()
    elif "```" in result:
        code = result.split("```")[1].split("```")[0].strip()

    print(f"\nğŸ” æ‰§è¡ŒæŸ¥è¯¢: {args.sql}")
    print("â”€" * 60)

    # Execute the code
    with tempfile.NamedTemporaryFile(mode="w", suffix=".py", delete=False) as f:
        f.write(code)
        tmp = f.name

    try:
        result = subprocess.run(
            [sys.executable, tmp],
            capture_output=True,
            text=True,
            timeout=30,
        )
        if result.stdout:
            print(result.stdout)
        if result.stderr:
            print(f"âš ï¸ {result.stderr[:300]}")
    except subprocess.TimeoutExpired:
        print("âŒ æŸ¥è¯¢æ‰§è¡Œè¶…æ—¶")
    finally:
        os.unlink(tmp)

    print("â”€" * 60)
    save_history("query", args.file, args.sql, code[:200])


def cmd_compare(args):
    """Compare two CSV files."""
    h1, d1, _ = load_csv(args.file1)
    h2, d2, _ = load_csv(args.file2)

    t1 = infer_column_types(h1, d1)
    t2 = infer_column_types(h2, d2)

    s1 = build_schema_prompt(h1, d1, t1)
    s2 = build_schema_prompt(h2, d2, t2)

    table1 = csv_to_markdown_table(h1, d1, max_rows=10)
    table2 = csv_to_markdown_table(h2, d2, max_rows=10)

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æå¸ˆã€‚è¯·æ¯”è¾ƒä»¥ä¸‹ä¸¤ä¸ªæ•°æ®é›†å¹¶ç»™å‡ºè¯¦ç»†åˆ†æã€‚

## æ•°æ®é›† 1: {args.file1}
{s1}
{table1}

## æ•°æ®é›† 2: {args.file2}
{s2}
{table2}

## è¯·åˆ†æ
1. ğŸ” **ç»“æ„å·®å¼‚** â€” åˆ—åã€ç±»å‹ã€æ•°é‡å¯¹æ¯”
2. ğŸ“Š **æ•°æ®å·®å¼‚** â€” æ•°å€¼èŒƒå›´ã€åˆ†å¸ƒã€è¶‹åŠ¿å¯¹æ¯”
3. ğŸ”— **å…±åŒç‚¹** â€” ç›¸åŒçš„åˆ—ã€å¯å…³è”çš„å­—æ®µ
4. ğŸ’¡ **æ´å¯Ÿ** â€” ä¸¤ä¸ªæ•°æ®é›†ç»“åˆåå¯ä»¥å¾—å‡ºä»€ä¹ˆç»“è®º
5. ğŸ› ï¸ **åˆå¹¶å»ºè®®** â€” å¦‚ä½•åˆå¹¶è¿™ä¸¤ä¸ªæ•°æ®é›†

ç”¨ä¸­æ–‡å›ç­”ï¼Œç”¨ markdown æ ¼å¼ã€‚"""

    print(f"\nğŸ”„ å¯¹æ¯”åˆ†æ: {args.file1} vs {args.file2}")
    print("â•" * 60)
    result = llm_query(prompt, timeout=90)
    print(result)
    print("â•" * 60)

    save_history("compare", f"{args.file1} vs {args.file2}", "compare", result)


def cmd_history(args):
    """Show query history."""
    if not HISTORY_FILE.exists():
        print("ğŸ“­ æš‚æ— å†å²è®°å½•")
        return

    history = json.loads(HISTORY_FILE.read_text())
    if args.clear:
        HISTORY_FILE.unlink()
        print("âœ… å†å²è®°å½•å·²æ¸…é™¤")
        return

    print(f"\nğŸ“œ æŸ¥è¯¢å†å² (æœ€è¿‘ {min(len(history), 20)} æ¡)")
    print("â”€" * 60)
    for entry in history[-20:]:
        ts = entry.get("timestamp", "?")[:19]
        action = entry.get("action", "?")
        file = Path(entry.get("file", "?")).name
        query = entry.get("query", "")[:50]
        emoji = {"ask": "â“", "report": "ğŸ“", "clean": "ğŸ§¹", "plot": "ğŸ“Š", "query": "ğŸ”", "compare": "ğŸ”„"}.get(action, "ğŸ“Œ")
        print(f"  {emoji} [{ts}] {action} on {file}: {query}")
    print("â”€" * 60)


# ---------------------------------------------------------------------------
# Main CLI
# ---------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(
        prog="csvwise",
        description="ğŸ§  csvwise - AI-Powered CSV Data Analyst",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  csvwise info data.csv                          # æŸ¥çœ‹æ•°æ®æ¦‚è§ˆ
  csvwise ask data.csv "å¹³å‡é”€å”®é¢æ˜¯å¤šå°‘?"          # æé—®
  csvwise report data.csv -o report.md            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
  csvwise clean data.csv                          # æ•°æ®æ¸…æ´—å»ºè®®
  csvwise plot data.csv "æŒ‰æœˆä»½çš„é”€å”®è¶‹åŠ¿"          # ç”Ÿæˆå›¾è¡¨
  csvwise query data.csv "é”€å”®é¢ > 10000 çš„è®°å½•"    # SQL å¼æŸ¥è¯¢
  csvwise compare a.csv b.csv                     # å¯¹æ¯”ä¸¤ä¸ªæ•°æ®é›†
  csvwise history                                 # æŸ¥çœ‹å†å²
        """,
    )
    parser.add_argument("--version", action="version", version=f"csvwise {VERSION}")

    sub = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")

    # info
    p_info = sub.add_parser("info", help="æŸ¥çœ‹æ•°æ®é›†æ¦‚è§ˆ")
    p_info.add_argument("file", help="CSV æ–‡ä»¶è·¯å¾„")

    # ask
    p_ask = sub.add_parser("ask", help="ç”¨è‡ªç„¶è¯­è¨€æé—®")
    p_ask.add_argument("file", help="CSV æ–‡ä»¶è·¯å¾„")
    p_ask.add_argument("question", help="ä½ çš„é—®é¢˜")

    # report
    p_report = sub.add_parser("report", help="ç”Ÿæˆå…¨é¢åˆ†ææŠ¥å‘Š")
    p_report.add_argument("file", help="CSV æ–‡ä»¶è·¯å¾„")
    p_report.add_argument("-o", "--output", help="ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶")

    # clean
    p_clean = sub.add_parser("clean", help="æ•°æ®æ¸…æ´—å»ºè®®")
    p_clean.add_argument("file", help="CSV æ–‡ä»¶è·¯å¾„")

    # plot
    p_plot = sub.add_parser("plot", help="ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
    p_plot.add_argument("file", help="CSV æ–‡ä»¶è·¯å¾„")
    p_plot.add_argument("description", help="å›¾è¡¨æè¿°")
    p_plot.add_argument("--run", action="store_true", help="è‡ªåŠ¨è¿è¡Œç»˜å›¾è„šæœ¬")

    # query
    p_query = sub.add_parser("query", help="SQL å¼æ•°æ®æŸ¥è¯¢")
    p_query.add_argument("file", help="CSV æ–‡ä»¶è·¯å¾„")
    p_query.add_argument("sql", help="æŸ¥è¯¢æè¿°")

    # compare
    p_compare = sub.add_parser("compare", help="å¯¹æ¯”ä¸¤ä¸ªæ•°æ®é›†")
    p_compare.add_argument("file1", help="ç¬¬ä¸€ä¸ª CSV æ–‡ä»¶")
    p_compare.add_argument("file2", help="ç¬¬äºŒä¸ª CSV æ–‡ä»¶")

    # history
    p_history = sub.add_parser("history", help="æŸ¥çœ‹æŸ¥è¯¢å†å²")
    p_history.add_argument("--clear", action="store_true", help="æ¸…é™¤å†å²è®°å½•")

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        sys.exit(0)

    commands = {
        "info": cmd_info,
        "ask": cmd_ask,
        "report": cmd_report,
        "clean": cmd_clean,
        "plot": cmd_plot,
        "query": cmd_query,
        "compare": cmd_compare,
        "history": cmd_history,
    }

    commands[args.command](args)


if __name__ == "__main__":
    main()
